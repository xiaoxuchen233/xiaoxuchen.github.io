---
title: 'Gibbs Sampling Note'
date: 2021-01-12
permalink: /posts/2021/01/blog-post-1/
tags:
  - Bayesian machine learning
  - Markov Chain Monte Carlo
  - Gibbs Sampling
---

In this post, I would like to record some notes about **Gibbs Sampling**, which is widely used as an **approximation inference** method for **Bayesian probabilistic graphical model**.

Gibbs Sampling
======
The Gibbs sampling ia applicable for sampling from a multivariate probability distribution $p(\mathbf{x})$.

Problem
---
Given a target distribution $p(\mathbf{x})$, where $\mathbf{x}=(x_{1},x_{2},...,x_{D})$, sampling $\mathbf{x}^{(1)},\mathbf{x}^{(2)},...,\mathbf{x}^{(n)}$.

Requirement
---
There are two requirements for using Gibbs sampling:
- analytic (mathematical) expressions for the conditional distribution of each variable given all other variables in the joint.
For D-dimensional target distribution $p(\mathbf{x})$, we can have D individual expressions:
$$p(x_{i}\mid x_{1},x_{2},...,x_{i-1},x_{i+1},...,x_{D})=p(x_{i}\mid x_{j}),j\neq i.$$
Each of these expressions defines the probability of the $i$-th dimension given that we have values for all other $(j\neq i)$ dimensions.
- we must be able to sample from each conditional distribution.

Sampling
---
Each iteration of the Gibbs sampler cycles through the subvectors of $\mathbf{x}$, drawing each subset conditional on the value of all the others. There are thus D steps in iteration t. At each iteration $t$, an ordering of the D subvectors of $\mathbf{x}$ is choosen and, in turn, each $x_{i}^{(t)}$ is sampled from the conditional distribution given all the other components of $\mathbf{x}$. The algorithm can be outlined below:

1. set $t=0$
2. generate an initial state $x_{i}^{(t)}~\pi^{(t)}$
3. repeat until $t=M$:

   &emsp;&emsp;set $t+=1$

   &emsp;&emsp;for each dimension $i=1,...,D$

      &emsp;&emsp;&emsp;&emsp;draw $x_{i}$ from $p(x_{i}\mid x_{1},x_{2},...,x_{i-1},x_{i+1},...,x_{D})$

Example: Sampling from a bivariate a Normal distribution
======
We will draw samples from 2-D Normal distribution using Gibbs Sampling.

The target distribution $p(\mathbf{x})$ with parameters like this form:

$$p(\mathbf{x})=\mathit{N}(\mathbf{x}\mid \mathbf{\mu,\Sigma})$$

where 

$$\mathbf{\mu}=[\mu_{1},\mu_{2}]=[0,0]$$ 

$$\mathbf{\Sigma}= \begin{bmatrix} 1 & \rho_{12}\\ \rho_{21}& 1\end{bmatrix}=\begin{bmatrix} 1 & 0.8\\ 0.8 & 1\end{bmatrix}$$

To use the Gibbs Sampling, we first need to get the conditional distribution for variables $x_{1},x_{2}$:

$p(x_{1}\mid x_{2}^{(t-1)})$ and $p(x_{2}\mid x_{1}^{t})$

Conditional distribution computation
---
- Marginal and conditional distributions of multivariate normal distribution

Assume an n-dimensional random vector

$$\mathbf{x}=\begin{bmatrix} \mathbf{x}_{1} \\ \mathbf{x}_{2} \end{bmatrix}$$

has a normal distribution $N({\bf x},{\bf \mu},{\bf \Sigma})$ with

$$\mathbf{\mu}=[{\bf \mu_{1},\mu_{2}}]$$ 

$$\mathbf{\Sigma}= \begin{bmatrix} \mathbf{\Sigma}_{11} & \mathbf{\Sigma}_{12}\\ \mathbf{\Sigma}_{21}& \mathbf{\Sigma}_{22}\end{bmatrix}$$

where $\mathbf{x_{1}}$ and $\mathbf{x_{2}}$ are two subvectors of respective dimensions $p$ and $q$ with $p+q=n$. Note that $\Sigma=\Sigma^{T}$, and $\Sigma_{21}=\Sigma_{21}^{T}$.

The marginal distributions of $\mathbf{x_{1}}$ and $\mathbf{x_{2}}$ are also normal with mean vector $\mu_{i}$ and covariance matrix $\Sigma_{ii}$ ($i=1,2$), respectively.

The conditional distribution of $\mathbf{x}_{i}$ given $\mathbf{x}_{j}$ is alsp normal with mean vector

$$\mu_{i\mid j}=\mu_{i}+\Sigma_{ij}\Sigma_{jj}^{-1}(\mathbf{x}_{j}-\mu_{j})$$

and covariance matrix

$$\Sigma_{i\mid j}=\Sigma_{jj}-\Sigma_{ij}^{T}\Sigma_{ii}^{-1}\Sigma_{ij}$$

Therefore, the two conditional distributions for 2-D Normal distribution are:

$$p(x_{1}\mid x_{2}^{(t-1)})=\mathit{N}(\mu_{1}+\rho_{21}(x_{2}^{(t-1)}-\mu_{2}),\sqrt{1-\rho_{21}^{2}})$$

$$p(x_{2}\mid x_{1}^{(t)})=\mathit{N}(\mu_{2}+\rho_{12}(x_{1}^{(t)}-\mu_{1}),\sqrt{1-\rho_{12}^{2}})$$

which are both univariate Normal distributions, each with a mean that is dependent on the value of the most recent state of the conditioning variable, and a variance that is dependent on the target covariances between the two variables.

Sampling Implement
---
```python
% EXAMPLE: GIBBS SAMPLER FOR BIVARIATE NORMAL
rand('seed' ,12345);
nSamples = 5000;
 
mu = [0 0]; % TARGET MEAN
rho(1) = 0.8; % rho_21
rho(2) = 0.8; % rho_12
 
% INITIALIZE THE GIBBS SAMPLER
propSigma = 1; % PROPOSAL VARIANCE
minn = [-3 -3];
maxx = [3 3];
 
% INITIALIZE SAMPLES
x = zeros(nSamples,2);
x(1,1) = unifrnd(minn(1), maxx(1));
x(1,2) = unifrnd(minn(2), maxx(2));
 
dims = 1:2; % INDEX INTO EACH DIMENSION
 
% RUN GIBBS SAMPLER
t = 1;
while t < nSamples
    t = t + 1;
    T = [t-1,t];
    for iD = 1:2 % LOOP OVER DIMENSIONS
        % UPDATE SAMPLES
        nIx = dims~=iD; % *NOT* THE CURRENT DIMENSION
        % CONDITIONAL MEAN
        muCond = mu(iD) + rho(iD)*(x(T(iD),nIx)-mu(nIx));
        % CONDITIONAL VARIANCE
        varCond = sqrt(1-rho(iD)^2);
        % DRAW FROM CONDITIONAL
        x(t,iD) = normrnd(muCond,varCond);
    end
end
 
% DISPLAY SAMPLING DYNAMICS
figure;
h1 = scatter(x(:,1),x(:,2),'r.');
 
% CONDITIONAL STEPS/SAMPLES
hold on;
for t = 1:50
    plot([x(t,1),x(t+1,1)],[x(t,2),x(t,2)],'k-');
    plot([x(t+1,1),x(t+1,1)],[x(t,2),x(t+1,2)],'k-');
    h2 = plot(x(t+1,1),x(t+1,2),'ko');
end
 
h3 = scatter(x(1,1),x(1,2),'go','Linewidth',3);
legend([h1,h2,h3],{'Samples','1st 50 Samples','x(t=0)'},'Location','Northwest')
hold off;
xlabel('x_1');
ylabel('x_2');
axis square
```
