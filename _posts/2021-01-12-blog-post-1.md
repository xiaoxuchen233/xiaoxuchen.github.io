---
title: 'Gibbs Sampling Note'
date: 2021-01-12
permalink: /posts/2021/01/blog-post-1/
tags:
  - Bayesian machine learning
  - Markov Chain Monte Carlo
  - Gibbs Sampling
---

In this post, I would like to record some notes about **Gibbs Sampling**, which is widely used as an **approximation inference** method for **Bayesian probabilistic graphical model**.

Gibbs Sampling
======
The Gibbs sampling ia applicable for sampling from a multivariate probability distribution $p(\mathbf{x})$.
Problem
---
Given a target distribution $p(\mathbf{x})$, where $\mathbf{x}=(x_{1},x_{2},...,x_{D})$, sampling $\mathbf{x}^{1},\mathbf{x}^{2},...,\mathbf{x}^{n}$.
Requirement
---
There are two requirements for using Gibbs sampling:
- analytic (mathematical) expressions for the conditional distribution of each variable given all other variables in the joint.
For D-dimensional target distribution $p(\mathbf{X})$, we can have D individual expressions:
$$p(x_{i}\mid x_{1},x_{2},...,x_{i-1},x_{i+1},...,x_{D})=p(x_{i}\mid x_{j}),j\neq i.$$
Each of these expressions defines the probability of the $i$-th dimension given that we have values for all other $(j\neq i)$ dimensions.
- we must be able to sample from each conditional distribution.
Sampling
---
Each iteration of the Gibbs sampler cycles through the subvectors of $/mathbf{x}$, drawing each subset conditional on the value of all the others. There are thus D steps in iteration t. At each iteration $t$, an ordering of the D subvectors of $/mathbf{x}$ is choosen and, in turn, each $x_{i}^{t}$ is sampled from the conditional distribution given all the other components of $/mathbf{x}$. The algorithm can be outlined below:

1. set $t=0$
&emsp 2.

Example:Sampling from a bivariate a Normal distribution
======
This example 
